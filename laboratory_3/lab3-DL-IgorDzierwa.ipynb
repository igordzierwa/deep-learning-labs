{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5c04ed",
   "metadata": {},
   "source": [
    "## Author: Igor Dzierwa\n",
    "## Laboratory 3 - Implementation of Gradient Descent in neural networks with one hidden layer\n",
    "\n",
    "In this lab we are going to implement a neural network with one hidden layer. The architecture of our neural network is as follows:\n",
    "\n",
    "- Input layer with 3 neurons\n",
    "- One hidden layer with 2 neurons\n",
    "- Output layer with 1 neuron\n",
    "\n",
    "The dataset has 200 data each one has three features selected from a random normal distribution with a mean of zero and the standard deviation of 5 and the actual labels 0 or 1.\n",
    "\n",
    "Weights are also initialized randomly from normal distribution with *Xavier* initialization technique. Hidden  layer activation is *Tanh* and the output layer activation is *Sigmoid* with *binary cross-entropy* as loss.\n",
    "\n",
    "Also, for this task use batch gradient descent algorithm.\n",
    "\n",
    "Please define different functions for each part of the training process:\n",
    "- initialization\n",
    "- activation functions and their derivative \n",
    "- loss function and its derivative \n",
    "- forward propagation\n",
    "- backpropagation \n",
    "- parameters update\n",
    "- training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6e734",
   "metadata": {},
   "source": [
    "#### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104f5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a216115",
   "metadata": {},
   "source": [
    "#### Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b27d842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_initialization(mean=0, sd=5, count=200):\n",
    "    sample_normal_dist = np.random.normal(mu, sigma, 1000)\n",
    "    \n",
    "    features_list = [random.choices(sample_normal_dist, k=3) for i in range(count)]\n",
    "    values_list = [1 if sum(i) >= 0 else 0 for i in features_list]\n",
    "    \n",
    "    return features_list, values_list\n",
    "\n",
    "dataset = dataset_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0115ca",
   "metadata": {},
   "source": [
    "#### Sigmoid and Tanh activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51063955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(self, x):\n",
    "    return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "def tanh(z):\n",
    "    ez = np.exp(z)\n",
    "    enz = np.exp(-z)\n",
    "    return (ez - enz)/ (ez + enz)\n",
    "\n",
    "def tanh_deriv(z):\n",
    "    a = tanh(z)\n",
    "    return 1 - a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411ecc5",
   "metadata": {},
   "source": [
    "#### Cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc89b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_formula(y, output):\n",
    "    return -y * np.log(output) - (1-y) * np.log(1-output)\n",
    "\n",
    "def error_term_frumula(x, yhat, y):\n",
    "    return (yhat - y)*x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
